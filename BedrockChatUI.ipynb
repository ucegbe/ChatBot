{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce80d69-2321-49fc-b34c-bca443c76a2a",
   "metadata": {},
   "source": [
    "This notebook was tested in a `ml.t3.medium` instance and Sagemaker`Data Science 3` image Studio Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18167e24-3077-4e66-af16-d1bce1f4643e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"chatbots.png\" width=\"800\"/>\n",
    "\n",
    "This sample notebooks implements a general chatbot.\n",
    "Key functionalities include:\n",
    "1. Saving of Conversation History in DynamoDB\n",
    "2. Handling Document upload for various supported document format (PDF, JPG, CSV, EXCEL, PNG, TXT, JSON) by passing the document local or S3 path.\n",
    "3. Implementing various prompt template store locally (can also be stored in S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217fbc3-0e61-4cf6-b61a-b138e9dbec8f",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146a09e-407e-44da-98b0-39bf89599065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install anthropic\n",
    "!pip install s3fs -U\n",
    "!pip install pandas -U\n",
    "!pip install --force-reinstall amazon-textract-textractor==1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ba6ed-5037-456f-89f9-5dd978b78ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from anthropic import Anthropic\n",
    "from botocore.config import Config\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import re\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from openpyxl.cell import Cell\n",
    "from openpyxl.worksheet.cell_range import CellRange\n",
    "import uuid\n",
    "from botocore.exceptions import ClientError\n",
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures\n",
    "from textractor.data.text_linearization_config import TextLinearizationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a59f0cb-7897-44ca-9754-80330c853e8e",
   "metadata": {},
   "source": [
    "#### Initialize Bedrock Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbfa50-c4a1-4fd3-8c68-4b205ba1d7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the bedrock runtime to invoke LLM\n",
    "from botocore.config import Config\n",
    "config = Config(\n",
    "    read_timeout=600, # Read timeout parameter\n",
    "    retries = dict(\n",
    "        max_attempts = 10 ## Handle retries\n",
    "    )\n",
    ")\n",
    "import boto3\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name='us-west-2',config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a615d7f-307f-4b79-a8d4-601e242906d7",
   "metadata": {},
   "source": [
    "Configurable:\n",
    "- `DYNAMODB_TABLE`: The name of the DynamoDB table used for storing chat history.\n",
    "- `DYNAMODB_USER`: The default user ID for the application (used if authentication is disabled).\n",
    "- `BUCKET`: The name of the S3 bucket used for caching documents and extracted text.\n",
    "- `CHAT_HISTORY_LENGTH`: The number of recent chat messages to load from the DynamoDB table.\n",
    "- `bedrock-region`: The AWS region where the Bedrock runtime is deployed.\n",
    "- `LOAD_DOC_IN_ALL_CHAT_CONVO`: A boolean flag indicating whether to load documents in the chat history.\n",
    "- `S3_DOC_CACHE_PATH`: S3 path to store attached document if from local system\n",
    "- `TEXTRACT_RESULT_CACHE_PATH`: S3 path to cache extracted PDF and Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad51c8-7eca-4482-a334-15c6358c8f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DYNAMODB_TABLE=\"SessionChatHistory\" \n",
    "DYNAMODB_USER= \"test-user-sonnet10\"\n",
    "SESSIONID=str(uuid.uuid4())\n",
    "DYNAMODB  = boto3.resource('dynamodb')\n",
    "dynamo=boto3.client('dynamodb')\n",
    "chat_hist=[]\n",
    "BUCKET=\"fairstone\"\n",
    "S3_DOC_CACHE_PATH='uploads'\n",
    "TEXTRACT_RESULT_CACHE_PATH=\"textract_output\"\n",
    "LOAD_DOC_IN_ALL_CHAT_CONVO=False\n",
    "CHAT_HISTORY_LENGTH=5\n",
    "S3=boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d49c0-58ae-4984-8c19-34c0e659e4cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create DynamoDB Table\n",
    "A DynamoDB Table is created with a user ID as partition Key and Session ID as sort key. \n",
    "This enables saving multiple chat session history under the same user id.\\\n",
    "Provide a bucket name that would be used to cache Amazon Textract results for document OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12996c34-511b-430b-8b73-e9ca8b838fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "try:\n",
    "    table = DYNAMODB.create_table(\n",
    "        TableName=DYNAMODB_TABLE,\n",
    "        KeySchema=[\n",
    "            {\n",
    "                'AttributeName': 'UserId',  # Partition key\n",
    "                'KeyType': 'HASH'  \n",
    "            },\n",
    "            {\n",
    "                'AttributeName': 'SessionId',   # Sort key\n",
    "                'KeyType': 'RANGE'\n",
    "            }\n",
    "        ],\n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                'AttributeName': 'UserId',\n",
    "                'AttributeType': 'S'   # String data type\n",
    "            },\n",
    "            {\n",
    "                'AttributeName': 'SessionId',\n",
    "                'AttributeType': 'S'\n",
    "            },\n",
    "        ],\n",
    "        BillingMode='PAY_PER_REQUEST'  # On-demand billing\n",
    "    )\n",
    "\n",
    "    print(\"Table status:\", table.table_status)\n",
    "\n",
    "    # Wait until the table exists.\n",
    "    table.meta.client.get_waiter(\"table_exists\").wait(TableName=\"SessionChatHistory\")\n",
    "    print(table.item_count)\n",
    "except dynamo.exceptions.ResourceInUseException as e:\n",
    "    print(e.response['Error']['Message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724ecf2-954f-4571-aa11-0b98558dffbc",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af03f8-f817-4d2f-829d-47fd821e5f06",
   "metadata": {},
   "source": [
    "This function reads an Excel file from the specified S3 bucket using the provided S3 URI.\n",
    "   It loads the workbook using openpyxl, unmerges any merged cells, and copies their values\n",
    "   to individual cells. The worksheet data is then converted to a pandas DataFrame, and the\n",
    "   `strip_newline` function is applied to each cell value to remove newline characters.\n",
    "   Finally, the DataFrame is converted to a CSV string with pipe (|) as the delimiter and\n",
    "   returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27929b3a-eae7-4abe-be72-f6932fb36184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strip_newline(cell):\n",
    "    return str(cell).strip()\n",
    "\n",
    "def table_parser_utills(file):        \n",
    "    # Read from S3\n",
    "    s3 = boto3.client('s3')\n",
    "    match = re.match(\"s3://(.+?)/(.+)\", file)\n",
    "    if match:\n",
    "        bucket_name = match.group(1)\n",
    "        key = match.group(2)    \n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)  \n",
    "    # Read Excel file from S3 into a buffer\n",
    "    xlsx_buffer = io.BytesIO(obj['Body'].read())\n",
    "    xlsx_buffer.seek(0) \n",
    "    # Load workbook, get active worksheet\n",
    "    wb = openpyxl.load_workbook(xlsx_buffer)\n",
    "    worksheet = wb.active    \n",
    "    for merged_cell_range in all_merged_cell_ranges:\n",
    "        merged_cell: Cell = merged_cell_range.start_cell\n",
    "        worksheet.unmerge_cells(range_string=merged_cell_range.coord)\n",
    "        for row_index, col_index in merged_cell_range.cells:\n",
    "            cell: Cell = worksheet.cell(row=row_index, column=col_index)\n",
    "            cell.value = merged_cell.value\n",
    "    # determine table header index\n",
    "    df = pd.DataFrame(worksheet.values)\n",
    "    df=df.map(strip_newline)  \n",
    "    return df.to_csv(sep=\"|\", index=False,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86bd7e-c44d-4496-bc5f-62f34b171e63",
   "metadata": {},
   "source": [
    " Retrieves a list of object keys from an S3 bucket with the specified prefix.\n",
    " \n",
    "   This function uses the boto3 library to interact with AWS S3. It lists the objects\n",
    "   in the specified bucket that have keys starting with the given prefix. The function\n",
    "   extracts the object keys, removes the prefix from each key, and appends the resulting\n",
    "   names to a list. If no objects are found with the specified prefix, an empty list is\n",
    "   returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52d892-c56e-4c33-af8f-94df9ba4df48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_s3_keys(prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET, Prefix=prefix)\n",
    "    keys=\"\"\n",
    "    if \"Contents\" in response:\n",
    "        keys = []\n",
    "        for obj in response['Contents']:\n",
    "            key = obj['Key']\n",
    "            name = key[len(prefix):]\n",
    "            keys.append(name)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56bf29-ed5f-4c66-b167-33a81f76da39",
   "metadata": {},
   "source": [
    "Extracts text from a PDF or image file using AWS Textract.\n",
    "\n",
    "   This function checks if the extracted text content for the given file is already cached in S3.\n",
    "   If cached, it retrieves the text from S3 and returns it. If not cached, it uses the Textractor\n",
    "   library to extract the text from the file using AWS Textract.\n",
    "\n",
    "   For PDF files, it makes an asynchronous call to start_document_analysis(), which may result in\n",
    "   some wait time. For other file types (e.g., images), it makes a synchronous call to analyze_document().\n",
    "\n",
    "   The extracted text is then processed using a TextLinearizationConfig to customize the output format.\n",
    "   The resulting text is uploaded to S3 for caching and then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ff2ff-c800-40c4-812b-60112b847e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exract_pdf_text_aws(file):    \n",
    "    file_base_name=os.path.basename(file)\n",
    "    # Checking if extracted doc content is in S3\n",
    "    if [x for x in get_s3_keys(f\"{TEXTRACT_RESULT_CACHE_PATH}/\") if file_base_name in x]:      \n",
    "        response = S3.get_object(Bucket=BUCKET, Key=f\"{TEXTRACT_RESULT_CACHE_PATH}/{file_base_name}.txt\")\n",
    "        text = response['Body'].read()\n",
    "        return text\n",
    "    else:\n",
    "        dir_name, ext = os.path.splitext(file)\n",
    "        extractor = Textractor(region_name=\"us-east-1\")\n",
    "        # Asynchronous call, you will experience some wait time. Try caching results for better experience\n",
    "        if \"pdf\" in ext:\n",
    "            print(\"Asynchronous call, you may experience some wait time.\")\n",
    "            document = extractor.start_document_analysis(\n",
    "            file_source=file,\n",
    "            features=[TextractFeatures.LAYOUT,TextractFeatures.TABLES],       \n",
    "            save_image=False,   \n",
    "            s3_output_path=f\"s3://{BUCKET}/textract_output/\"\n",
    "        )\n",
    "        #Synchronous call\n",
    "        else:\n",
    "            document = extractor.analyze_document(\n",
    "            file_source=file,\n",
    "            features=[TextractFeatures.LAYOUT,TextractFeatures.TABLES],  \n",
    "            save_image=False,\n",
    "        )\n",
    "        config = TextLinearizationConfig(\n",
    "        hide_figure_layout=True,   \n",
    "        hide_header_layout=False,    \n",
    "        table_prefix=\"<table>\",\n",
    "        table_suffix=\"</table>\",\n",
    "        )\n",
    "        # Upload extracted content to s3\n",
    "        S3.put_object(Body=document.get_text(config=config), Bucket=BUCKET, Key=f\"{TEXTRACT_RESULT_CACHE_PATH}/{file_base_name}.txt\") \n",
    "        return document.get_text(config=config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75127a3e-55f8-44a0-9e29-f2c5f6891e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_s3_obj_from_bucket_(file):\n",
    "    \"\"\"Retrieves an object from an S3 bucket given its S3 URI.\n",
    "    Args:\n",
    "       file (str): The S3 URI of the object to retrieve, in the format \"s3://{bucket_name}/{key}\".\n",
    "   Returns:\n",
    "       botocore.response.StreamingBody: The retrieved S3 object.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    match = re.match(\"s3://(.+?)/(.+)\", file)\n",
    "    if match:\n",
    "        bucket_name = match.group(1)\n",
    "        key = match.group(2)    \n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)  \n",
    "    return obj\n",
    "\n",
    "def put_obj_in_s3_bucket_(docs):\n",
    "    \"\"\"Uploads a file to an S3 bucket and returns the S3 URI of the uploaded object.\n",
    "    Args:\n",
    "       docs (str): The local file path of the file to upload to S3.\n",
    "   Returns:\n",
    "       str: The S3 URI of the uploaded object, in the format \"s3://{bucket_name}/{file_path}\".\n",
    "    \"\"\"\n",
    "    file_name=os.path.basename(docs)\n",
    "    file_path=f\"{S3_DOC_CACHE_PATH}/{file_name}\"\n",
    "    S3.upload_file(docs, BUCKET, file_path)\n",
    "    return f\"s3://{BUCKET}/{file_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ce327-dff3-4414-abbd-5ad21bf7f8b6",
   "metadata": {},
   "source": [
    "Handles the processing of uploaded documents or files from S3 based on their file extension.\n",
    "\n",
    "   This function takes a file path or S3 URI as input and processes the file based on its extension.\n",
    "   It supports the following file types:\n",
    "   - PDF, PNG, JPG: Extracts text from the file using the `extract_pdf_text_aws` function.\n",
    "   - CSV: Reads the file using pandas' `read_csv` function.\n",
    "   - XLSX, XLX: Parses the file using the `table_parser_utils` function.\n",
    "   - JSON: Retrieves the object from S3 using the `get_s3_obj_from_bucket_` function and loads the JSON content.\n",
    "   - TXT, PY: Retrieves the object from S3 using the `get_s3_obj_from_bucket_` function and reads the content.\n",
    "\n",
    "   The function can be extended to handle additional file extensions by implementing the corresponding logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c2ca3-0874-4da2-94d9-3c02daeeb233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_doc_upload_or_s3(file):\n",
    "    dir_name, ext = os.path.splitext(file)\n",
    "    if  ext in [\".pdf\", \".png\", \".jpg\"]:   \n",
    "        content=exract_pdf_text_aws(file)\n",
    "    elif \"csv\"  in ext:\n",
    "        content= pd.read_csv(file)\n",
    "    elif ext in [\".xlsx\", \".xlx\"]:\n",
    "        content=table_parser_utills(file)   \n",
    "    elif  \"json\" in ext:      \n",
    "        obj=get_s3_obj_from_bucket_(file)\n",
    "        content = json.loads(obj['Body'].read())  \n",
    "    elif  ext in [\".txt\",\".py\"]:       \n",
    "        obj=get_s3_obj_from_bucket_(file)\n",
    "        content = obj['Body'].read()\n",
    "    # Implement any of file extension logic \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc01092-7b04-4613-951d-67e4e6d32d7a",
   "metadata": {},
   "source": [
    "Stores long-term chat history in DynamoDB.\n",
    "\n",
    "   This function takes a dictionary of messages and stores it in a DynamoDB table.\n",
    "   It uses the user ID and session ID as the primary key to identify the item in the table.\n",
    "   If an item with the same user ID and session ID already exists in the table, the function\n",
    "   retrieves the existing messages and appends the new messages to the list.\n",
    "   Finally, it puts the updated chat item back into the DynamoDB table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fdc72-92da-4158-8cbe-b8220a8cd431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def put_db(messages):\n",
    "    \"\"\"Store long term chat history in DynamoDB\"\"\"    \n",
    "    chat_item = {\n",
    "        \"UserId\": DYNAMODB_USER, # user id\n",
    "        \"SessionId\": SESSIONID, # User session id\n",
    "        \"messages\": [messages],  # 'messages' is a list of dictionaries\n",
    "        \"time\":messages['time']\n",
    "    }\n",
    "    existing_item = DYNAMODB.Table(DYNAMODB_TABLE).get_item(Key={\"UserId\": DYNAMODB_USER, \"SessionId\":SESSIONID})\n",
    "    if \"Item\" in existing_item:\n",
    "        existing_messages = existing_item[\"Item\"][\"messages\"]\n",
    "        chat_item[\"messages\"] = existing_messages + [messages]\n",
    "    response = DYNAMODB.Table(DYNAMODB_TABLE).put_item(\n",
    "        Item=chat_item\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0c626-b5ca-4352-ab15-3aa93438aab0",
   "metadata": {},
   "source": [
    "Retrieves chat history from DynamoDB and prepares it for the conversation.\n",
    "\n",
    "   This function retrieves the chat history from DynamoDB based on the provided `chat_histories`\n",
    "   and `cutoff` parameters. It processes the chat history and prepares it for the conversation\n",
    "   based on the `claude3` flag and the `LOAD_DOC_IN_ALL_CHAT_CONVO` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c42fff-d32e-4454-94ac-53fc36508da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_chat_history_db(chat_histories, cutoff,claude3):\n",
    "    current_chat=[]\n",
    "    chat_hist=chat_histories['Item']['messages'][-cutoff:]            \n",
    "    for d in chat_hist:\n",
    "        if d['image'] and claude3 and LOAD_DOC_IN_ALL_CHAT_CONVO:\n",
    "            content=[]\n",
    "            for img in d['image']:\n",
    "                s3 = boto3.client('s3')\n",
    "                match = re.match(\"s3://(.+?)/(.+)\", img)\n",
    "                image_name=os.path.basename(img)\n",
    "                _,ext=os.path.splitext(image_name)\n",
    "                if \"jpg\" in ext: ext=\".jpeg\"                        \n",
    "                if match:\n",
    "                    bucket_name = match.group(1)\n",
    "                    key = match.group(2)    \n",
    "                    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                    base_64_encoded_data = base64.b64encode(obj['Body'].read())\n",
    "                    base64_string = base_64_encoded_data.decode('utf-8')                        \n",
    "                content.extend([{\"type\":\"text\",\"text\":image_name},{\n",
    "                  \"type\": \"image\",\n",
    "                  \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": f\"image/{ext.lower().replace('.','')}\",\n",
    "                    \"data\": base64_string\n",
    "                  }\n",
    "                }])\n",
    "            content.extend([{\"type\":\"text\",\"text\":d['user']}])\n",
    "            current_chat.append({'role': 'user', 'content': content})\n",
    "        elif d['document'] and LOAD_DOC_IN_ALL_CHAT_CONVO:\n",
    "            doc='Here are the documents:\\n'\n",
    "            for docs in d['document']:\n",
    "                uploads=handle_doc_upload_or_s3(docs)\n",
    "                doc_name=os.path.basename(docs)\n",
    "                doc+=f\"<{doc_name}>\\n{uploads}\\n</{doc_name}>\\n\"\n",
    "            if not claude3 and d[\"image\"]:\n",
    "                for docs in d['image']:\n",
    "                    uploads=handle_doc_upload_or_s3(docs)\n",
    "                    doc_name=os.path.basename(docs)\n",
    "                    doc+=f\"<{doc_name}>\\n{uploads}\\n</{doc_name}>\\n\"\n",
    "            current_chat.append({'role': 'user', 'content': [{\"type\":\"text\",\"text\":doc+d['user']}]})\n",
    "        else:\n",
    "            current_chat.append({'role': 'user', 'content': [{\"type\":\"text\",\"text\":d['user']}]})\n",
    "        current_chat.append({'role': 'assistant', 'content': d['assistant']})  \n",
    "    return current_chat, chat_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbf370-d81f-4a8e-b9df-cfca4ba311e5",
   "metadata": {},
   "source": [
    "Processes the streamed response from the Bedrock model and extracts the generated text.\n",
    "\n",
    "Invokes the Bedrock Claude model with the provided chat history, system message, prompt, and optional image(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbb611-737c-4915-8dc3-15925091cca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bedrock_streemer(response):\n",
    "    stream = response.get('body')\n",
    "    answer = \"\"\n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if  chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                if \"delta\" in chunk_obj:                    \n",
    "                    delta = chunk_obj['delta']\n",
    "                    if \"text\" in delta:\n",
    "                        text=delta['text'] \n",
    "                        print(text, end=\"\")\n",
    "                        answer+=str(text)       \n",
    "                        i+=1\n",
    "                if \"amazon-bedrock-invocationMetrics\" in chunk_obj:\n",
    "                    input_tokens= chunk_obj['amazon-bedrock-invocationMetrics']['inputTokenCount']\n",
    "                    output_tokens=chunk_obj['amazon-bedrock-invocationMetrics']['outputTokenCount']\n",
    "                    print(f\"\\nInput Tokens: {input_tokens}\\nOutput Tokens: {output_tokens}\")\n",
    "    return answer,input_tokens, output_tokens\n",
    "\n",
    "def bedrock_claude_(chat_history,system_message, prompt,model_id,image_path=None):\n",
    "    content=[{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "            }]\n",
    "    if image_path:       \n",
    "        if not isinstance(image_path, list):\n",
    "            image_path=[image_path]      \n",
    "        for img in image_path:\n",
    "            s3 = boto3.client('s3')\n",
    "            match = re.match(\"s3://(.+?)/(.+)\", img)\n",
    "            image_name=os.path.basename(img)\n",
    "            _,ext=os.path.splitext(image_name)\n",
    "            if \"jpg\" in ext: ext=\".jpeg\"                        \n",
    "            if match:\n",
    "                bucket_name = match.group(1)\n",
    "                key = match.group(2)    \n",
    "                obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                base_64_encoded_data = base64.b64encode(obj['Body'].read())\n",
    "                base64_string = base_64_encoded_data.decode('utf-8')\n",
    "            content.extend([{\"type\":\"text\",\"text\":image_name},{\n",
    "              \"type\": \"image\",\n",
    "              \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": f\"image/{ext.lower().replace('.','')}\",\n",
    "                \"data\": base64_string\n",
    "              }\n",
    "            }])\n",
    "    chat_history.append({\"role\": \"user\",\n",
    "            \"content\": content})\n",
    "    prompt = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1500,\n",
    "        \"temperature\": 0.5,\n",
    "        \"system\":system_message,\n",
    "        \"messages\": chat_history\n",
    "    }\n",
    "    answer = \"\"\n",
    "    prompt = json.dumps(prompt)\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(body=prompt, modelId=model_id, accept=\"application/json\", contentType=\"application/json\")\n",
    "    answer,input_tokens,output_tokens=bedrock_streemer(response) \n",
    "    return answer, input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d250fd-ae18-40e3-b799-6b08c905a435",
   "metadata": {},
   "source": [
    "Invokes the Bedrock Claude model with retries and exponential backoff in case of throttling errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6d59a-ae7e-4b73-93cd-eb385315c476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _invoke_bedrock_with_retries(current_chat, chat_template, question, model_id, image_path):\n",
    "    max_retries = 5\n",
    "    backoff_base = 2\n",
    "    max_backoff = 3  # Maximum backoff time in seconds\n",
    "    retries = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response,input_tokens,output_tokens = bedrock_claude_(current_chat, chat_template, question, model_id, image_path)\n",
    "            return response,input_tokens,output_tokens\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                if retries < max_retries:\n",
    "                    # Throttling, exponential backoff\n",
    "                    sleep_time = min(max_backoff, backoff_base ** retries + random.uniform(0, 1))\n",
    "                    time.sleep(sleep_time)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    raise e\n",
    "            else:\n",
    "                # Some other API error, rethrow\n",
    "                raise\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d1444e-a734-4a80-bce2-f51730af9517",
   "metadata": {},
   "source": [
    "#### Chat Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee7ea26-8cb1-471c-867a-fb028125d094",
   "metadata": {},
   "source": [
    "Conducts a conversation with the Bedrock Claude model based on the user's question and optional uploaded documents.\n",
    "\n",
    "   This function takes a user's question and a list of document paths (optional) as input. It retrieves the past chat\n",
    "   history from DynamoDB (if configured) or uses local memory storage. It prepares the chat template based on whether\n",
    "   documents are provided or not. If documents are provided, it handles the document uploads and extracts the text\n",
    "   content. If the Claude3 model is used, it handles images separately. The function then invokes the Bedrock Claude\n",
    "   model with retries and exponential backoff in case of throttling errors. The conversation history is stored in\n",
    "   DynamoDB (if configured) or local memory for future reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c188da-5167-4495-b951-f95ea40a59d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def conversation_bedroc_chat_(question, model_id,upload_doc: List[str]):\n",
    "    \"\"\"\n",
    "    Function takes a user query and a document path (from S3 or Local)\n",
    "    passing a document path is optional\n",
    "    \"\"\"    \n",
    "    num_retries=0\n",
    "    if not isinstance(upload_doc, list):\n",
    "        raise TypeError(\"documents must be in a list format\")\n",
    "        \n",
    "    # Check if Claude3 model is used and handle images with the CLAUDE3 Model\n",
    "    claude3=False\n",
    "    if \"sonnet\" in model_id or \"haiku\" in model_id:\n",
    "        claude3=True\n",
    "    current_chat=[]\n",
    "   \n",
    "    # Retrieve past chat history from Dynamodb\n",
    "    if DYNAMODB_TABLE:\n",
    "        chat_histories = DYNAMODB.Table(DYNAMODB_TABLE).get_item(Key={\"UserId\": DYNAMODB_USER, \"SessionId\":SESSIONID})\n",
    "        if \"Item\" in chat_histories:            \n",
    "            current_chat,chat_hist=get_chat_history_db(chat_histories, CHAT_HISTORY_LENGTH,claude3)\n",
    "        else:\n",
    "            chat_hist=[]\n",
    "    else:\n",
    "        for d in chat_hist:\n",
    "            current_chat.append({'role': 'user', 'content': d['user']})\n",
    "            current_chat.append({'role': 'assistant', 'content': d['assistant']})\n",
    "    ## prompt template for when a user uploads a doc\n",
    "    doc_path=[]\n",
    "    image_path=[]\n",
    "    doc=\"\"\n",
    "    if upload_doc:  \n",
    "        doc='Here are the documents:\\n'\n",
    "        for ids,docs in enumerate(upload_doc):\n",
    "            _,extensions=os.path.splitext(docs)\n",
    "            if not docs.startswith(\"s3://\"):\n",
    "                docs=put_obj_in_s3_bucket_(docs)\n",
    "            if extensions in [\".jpg\",\".jpeg\",\".png\",\".gif\",\".webp\"] and claude3:       \n",
    "                image_path.append(docs)\n",
    "                continue\n",
    "            uploads=handle_doc_upload_or_s3(docs)             \n",
    "            doc_path.append(docs)\n",
    "            doc_name=os.path.basename(docs)\n",
    "            doc+=f\"<{doc_name}>\\n{uploads}\\n</{doc_name}>\\n\"\n",
    "        with open(\"prompt/doc_chat.txt\",\"r\") as f:\n",
    "            chat_template=f.read()       \n",
    "    else:        \n",
    "        # Chat template for open ended query\n",
    "        with open(\"prompt/chat.txt\",\"r\") as f:\n",
    "            chat_template=f.read()    \n",
    "    response,input_tokens,output_tokens=_invoke_bedrock_with_retries(current_chat, chat_template, doc+question, model_id, image_path)\n",
    "    chat_history={\"user\":question,\n",
    "    \"assistant\":response,\n",
    "    \"image\":image_path,\n",
    "    \"document\":doc_path,\n",
    "    \"modelID\":model_id,\n",
    "    \"time\":str(time.time()),\n",
    "    \"input_token\":round(input_tokens) ,\n",
    "    \"output_token\":round(output_tokens)}         \n",
    "                 \n",
    "    #store convsation memory in DynamoDB table\n",
    "    if DYNAMODB_TABLE:\n",
    "        put_db(chat_history)\n",
    "    # use local memory for storage\n",
    "    else:\n",
    "        chat_hist.append(chat_history)   \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e31287-db05-4e8a-86bc-c46ce91da664",
   "metadata": {},
   "source": [
    "#### Query the the chat bot with your questions.\n",
    "Also takes a document path(s) stored in s3 or local. Once a documents path is passed, a different prompt template is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2f81b-4c73-4de1-b4c8-2c051914e3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question=\"\"\"Describe this app\"\"\"\n",
    "model_id=\"anthropic.claude-3-haiku-20240307-v1:0\"#\"anthropic.claude-3-sonnet-20240229-v1:0\"\"anthropic.claude-v2\",\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "docu=[\"bedrock-chat.py\",\"config.json\",\"pricing.json\"]\n",
    "res=conversation_bedroc_chat_(question, model_id,docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a71daa-6e72-4de1-80e5-a1f8299f5d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
